#!/bin/bash

# Set variables
DATA_DIR=$1
OUTPUT_DIR=$2

# Define random seed function
get_seeded_random() { seed="$1";  openssl enc -aes-256-ctr -pass pass:"$seed" -nosalt </dev/zero 2>/dev/null; }


# Preprocess data
echo "Training SentencePiece model..."
spm_train --input=$DATA_DIR/train.src,$DATA_DIR/train.trg \
          --model_prefix=$DATA_DIR/spm \
          --vocab_size=8000 \
          --character_coverage=1 \
          --model_type=bpe \
          --shuffle_input_sentence=True \
          --user_defined_symbols @@GFM@@

python3 scripts/convert_spm_vocab.py $DATA_DIR/spm.vocab

echo "Preprocessing trainset..."
spm_encode --model=$DATA_DIR/spm.model < $DATA_DIR/train.src > $DATA_DIR/train.bpe.src
spm_encode --model=$DATA_DIR/spm.model < $DATA_DIR/train.trg > $DATA_DIR/train.bpe.trg

echo "Shuffling processed trainsets..."
shuf --random-source=<(get_seeded_random 1234) $DATA_DIR/train.bpe.src > $DATA_DIR/shuf.train.bpe.src
shuf --random-source=<(get_seeded_random 1234) $DATA_DIR/train.bpe.trg > $DATA_DIR/shuf.train.bpe.trg

echo "Preprocessing validset..."
spm_encode --model=$DATA_DIR/spm.model < $DATA_DIR/valid.src > $DATA_DIR/valid.bpe.src
spm_encode --model=$DATA_DIR/spm.model < $DATA_DIR/valid.trg > $DATA_DIR/valid.bpe.trg


# Prepare training data
python -m sockeye.prepare_data \
--source-vocab $DATA_DIR/spm.vocab.json \
--target-vocab $DATA_DIR/spm.vocab.json \
-s $DATA_DIR/shuf.train.bpe.src \
-t $DATA_DIR/shuf.train.bpe.trg  \
--pad-vocab-to-multiple-of 8 --bucket-width 8 --shared-vocab \
--max-seq-len 143 \
--max-processes 20 \
-o $DATA_DIR/sockeye_train_data_prepared


# Run training
PARAMS=$OUTPUT_DIR/params.best

echo "Training..."

torchrun --no_python --nproc_per_node 1 sockeye-train \
    --prepared-data $DATA_DIR/sockeye_train_data_prepared \
    --validation-source $DATA_DIR/valid.bpe.src \
    --validation-target $DATA_DIR/valid.bpe.trg \
    --output $OUTPUT_DIR \
    --num-embed 512 \
    --num-layers=6:6 \
    --transformer-model-size 512 \
    --transformer-attention-heads 4 \
    --transformer-feed-forward-num-hidden 1024 \
    --amp \
    --batch-type max-word \
    --batch-size 10000 \
    --update-interval 10 \
    --checkpoint-interval 500 \
    --optimizer-betas 0.9:0.98 \
    --dist \
    --initial-learning-rate 0.0005 \
    --learning-rate-scheduler-type inv-sqrt-decay \
    --learning-rate-warmup 4000 \
    --max-num-checkpoint-not-improved=8 \
    --seed 1 \
    --decode-and-evaluate -1 \
    --max-num-epochs 30 \
    --keep-last-params=2 \
    --max-seq-len=143 \
    --quiet-secondary-workers \
    --env=PYTORCH_JIT=0


echo "Training done..."

# Exit success
exit 0
